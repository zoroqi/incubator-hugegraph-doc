<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>HugeGraph – HugeGraph-API Performance</title><link>/docs/performance/api-preformance/</link><description>Recent content in HugeGraph-API Performance on HugeGraph</description><generator>Hugo -- gohugo.io</generator><atom:link href="/docs/performance/api-preformance/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: v0.5.6 Stand-alone(RocksDB)</title><link>/docs/performance/api-preformance/hugegraph-api-0.5.6-rocksdb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/performance/api-preformance/hugegraph-api-0.5.6-rocksdb/</guid><description>
&lt;h3 id="1-test-environment">1 Test environment&lt;/h3>
&lt;p>Compressed machine information:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>CPU&lt;/th>
&lt;th>Memory&lt;/th>
&lt;th>网卡&lt;/th>
&lt;th>磁盘&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>48 Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz&lt;/td>
&lt;td>128G&lt;/td>
&lt;td>10000Mbps&lt;/td>
&lt;td>750GB SSD,2.7T HDD&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>Information about the machine used to generate load: configured the same as the machine that is being tested under load.&lt;/li>
&lt;li>Testing tool: Apache JMeter 2.5.1&lt;/li>
&lt;/ul>
&lt;p>Note: The load-generating machine and the machine under test are located in the same local network.&lt;/p>
&lt;h3 id="2-test-description">2 Test description&lt;/h3>
&lt;h4 id="21-definition-of-terms-the-unit-of-time-is-ms">2.1 Definition of terms (the unit of time is ms)&lt;/h4>
&lt;ul>
&lt;li>Samples: The total number of threads completed in the current scenario.&lt;/li>
&lt;li>Average: The average response time.&lt;/li>
&lt;li>Median: The statistical median of the response time.&lt;/li>
&lt;li>90% Line: The response time below which 90% of all threads fall.&lt;/li>
&lt;li>Min: The minimum response time.&lt;/li>
&lt;li>Max: The maximum response time.&lt;/li>
&lt;li>Error: The error rate.&lt;/li>
&lt;li>Throughput: The number of requests processed per unit of time.&lt;/li>
&lt;li>KB/sec: Throughput measured in terms of data transferred per second.&lt;/li>
&lt;/ul>
&lt;h4 id="22-underlying-storage">2.2 Underlying storage&lt;/h4>
&lt;p>RocksDB is used for backend storage, HugeGraph and RocksDB are both started on the same machine, and the configuration files related to the server remain as default except for the modification of the host and port.&lt;/p>
&lt;h3 id="3-summary-of-performance-results">3 Summary of performance results&lt;/h3>
&lt;ol>
&lt;li>The speed of inserting a single vertex and edge in HugeGraph is about 1w per second&lt;/li>
&lt;li>The batch insertion speed of vertices and edges is much faster than the single insertion speed&lt;/li>
&lt;li>The concurrency of querying vertices and edges by id can reach more than 13000, and the average delay of requests is less than 50ms&lt;/li>
&lt;/ol>
&lt;h3 id="4-test-results-and-analysis">4 Test results and analysis&lt;/h3>
&lt;h4 id="41-batch-insertion">4.1 batch insertion&lt;/h4>
&lt;h5 id="411-upper-limit-stress-testing">4.1.1 Upper limit stress testing&lt;/h5>
&lt;h6 id="test-methods">Test methods&lt;/h6>
&lt;p>The upper limit of stress testing is to continuously increase the concurrency and test whether the server can still provide services normally.&lt;/p>
&lt;h6 id="stress-parameters">Stress Parameters&lt;/h6>
&lt;p>Duration: 5 minutes&lt;/p>
&lt;h6 id="maximum-insertion-speed-for-vertices">Maximum insertion speed for vertices:&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/rocksdb/vertex_batch.png" alt="image">
&lt;/center>
&lt;p>####### in conclusion:&lt;/p>
&lt;ul>
&lt;li>With a concurrency of 2200, the throughput for vertices is 2026.8. This means that the system can process data at a rate of 405360 per second (2026.8 * 200).&lt;/li>
&lt;/ul>
&lt;h6 id="maximum-insertion-speed-for-edges">Maximum insertion speed for edges&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/rocksdb/edge_batch.png" alt="image">
&lt;/center>
&lt;p>####### Conclusion:&lt;/p>
&lt;ul>
&lt;li>With a concurrency of 900, the throughput for edges is 776.9. This means that the system can process data at a rate of 388450 per second (776.9 * 500).&lt;/li>
&lt;/ul>
&lt;h4 id="42-single-insertion">4.2 Single insertion&lt;/h4>
&lt;h5 id="421-stress-limit-testing">4.2.1 Stress limit testing&lt;/h5>
&lt;h6 id="test-methods-1">Test Methods&lt;/h6>
&lt;p>Stress limit testing is a process of continuously increasing the concurrency level to test the upper limit of the server&amp;rsquo;s ability to provide normal service.&lt;/p>
&lt;h6 id="stress-parameters-1">Stress parameters&lt;/h6>
&lt;ul>
&lt;li>Duration: 5 minutes.&lt;/li>
&lt;li>Service exception indicator: Error rate greater than 0.00%.&lt;/li>
&lt;/ul>
&lt;h6 id="single-vertex-insertion">Single vertex insertion&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/rocksdb/vertex_single.png" alt="image">
&lt;/center>
&lt;p>####### Conclusion:&lt;/p>
&lt;ul>
&lt;li>With a concurrency of 11500, the throughput is 10730. This means that the system can handle a single concurrent insertion of vertices at a concurrency level of 11500.&lt;/li>
&lt;/ul>
&lt;h6 id="single-edge-insertion">Single edge insertion&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/rocksdb/edge_single.png" alt="image">
&lt;/center>
&lt;p>####### Conclusion:&lt;/p>
&lt;ul>
&lt;li>With a concurrency of 9000, the throughput is 8418. This means that the system can handle a single concurrent insertion of edges at a concurrency level of 9000.&lt;/li>
&lt;/ul>
&lt;h4 id="43-search-by-id">4.3 Search by ID&lt;/h4>
&lt;h5 id="431-stress-test-upper-limit">4.3.1 Stress test upper limit&lt;/h5>
&lt;h6 id="testing-method">Testing method&lt;/h6>
&lt;p>Continuously increasing the concurrency level to test the upper limit of the server&amp;rsquo;s ability to provide service under normal conditions.&lt;/p>
&lt;h6 id="stress-parameters-2">stress parameters&lt;/h6>
&lt;ul>
&lt;li>Duration: 5 minutes&lt;/li>
&lt;li>Service abnormality indicator: error rate greater than 0.00%&lt;/li>
&lt;/ul>
&lt;h6 id="querying-vertices-by-id">Querying vertices by ID&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/rocksdb/vertex_id_query.png" alt="image">
&lt;/center>
&lt;p>####### Conclusion：&lt;/p>
&lt;ul>
&lt;li>Concurrency is 14,000, throughput is 12,663. The concurrency capacity for querying vertices by ID is 14,000, with an average delay of 44ms.&lt;/li>
&lt;/ul>
&lt;h6 id="querying-edges-by-id">Querying edges by ID&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/rocksdb/edge_id_query.png" alt="image">
&lt;/center>
&lt;p>####### Conclusion：&lt;/p>
&lt;ul>
&lt;li>Concurrency is 13,000, throughput is 12,225. The concurrency capacity for querying edges by ID is 13,000, with an average delay of 12ms.&lt;/li>
&lt;/ul></description></item><item><title>Docs: v0.5.6 Cluster(Cassandra)</title><link>/docs/performance/api-preformance/hugegraph-api-0.5.6-cassandra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/performance/api-preformance/hugegraph-api-0.5.6-cassandra/</guid><description>
&lt;h3 id="1-test-environment">1 Test environment&lt;/h3>
&lt;p>Compressed machine information&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>CPU&lt;/th>
&lt;th>Memory&lt;/th>
&lt;th>网卡&lt;/th>
&lt;th>磁盘&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>48 Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz&lt;/td>
&lt;td>128G&lt;/td>
&lt;td>10000Mbps&lt;/td>
&lt;td>750GB SSD,2.7T HDD&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>Starting Pressure Machine Information: Configured the same as the compressed machine.&lt;/li>
&lt;li>Testing tool: Apache JMeter 2.5.1.&lt;/li>
&lt;/ul>
&lt;p>Note: The machine used to initiate the load and the machine being tested are located in the same data center (or server room)&lt;/p>
&lt;h3 id="2-test-description">2 Test Description&lt;/h3>
&lt;h4 id="21-definition-of-terms-the-unit-of-time-is-ms">2.1 Definition of terms (the unit of time is ms)&lt;/h4>
&lt;ul>
&lt;li>Samples &amp;ndash; The total number of threads completed in this scenario.&lt;/li>
&lt;li>Average &amp;ndash; The average response time.&lt;/li>
&lt;li>Median &amp;ndash; The median response time in statistical terms.&lt;/li>
&lt;li>90% Line &amp;ndash; The response time below which 90% of all threads fall.&lt;/li>
&lt;li>Min &amp;ndash; The minimum response time.&lt;/li>
&lt;li>Max &amp;ndash; The maximum response time.&lt;/li>
&lt;li>Error &amp;ndash; The error rate.&lt;/li>
&lt;li>Throughput &amp;ndash; The number of transactions processed per unit of time.&lt;/li>
&lt;li>KB/sec &amp;ndash; The throughput measured in terms of data transmitted per second.&lt;/li>
&lt;/ul>
&lt;h4 id="22-low-level-storage">2.2 Low-Level Storage&lt;/h4>
&lt;p>A 15-node Cassandra cluster is used for backend storage. HugeGraph and the Cassandra cluster are located on separate servers. Server-related configuration files are modified only for host and port settings, while the rest remain default.&lt;/p>
&lt;h3 id="3-summary-of-performance-results">3 Summary of Performance Results&lt;/h3>
&lt;ol>
&lt;li>The speed of single vertex and edge insertion in HugeGraph is 9000 and 4500 per second, respectively.&lt;/li>
&lt;li>The speed of bulk vertex and edge insertion is 50,000 and 150,000 per second, respectively, which is much higher than the single insertion speed.&lt;/li>
&lt;li>The concurrency for querying vertices and edges by ID can reach more than 12,000, and the average request delay is less than 70ms.&lt;/li>
&lt;/ol>
&lt;h3 id="4-test-results-and-analysis">4 Test Results and Analysis&lt;/h3>
&lt;h4 id="41-batch-insertion">4.1 Batch Insertion&lt;/h4>
&lt;h5 id="411-pressure-upper-limit-test">4.1.1 Pressure Upper Limit Test&lt;/h5>
&lt;h6 id="test-method">Test Method&lt;/h6>
&lt;p>Continuously increase the concurrency level to test the upper limit of the server&amp;rsquo;s ability to provide services.&lt;/p>
&lt;h6 id="pressure-parameters">Pressure Parameters&lt;/h6>
&lt;p>Duration: 5 minutes.&lt;/p>
&lt;h6 id="maximum-insertion-speed-of-vertices">Maximum Insertion Speed of Vertices:&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/cassandra/vertex_batch.png" alt="image">
&lt;/center>
&lt;h6 id="conclusion">Conclusion:&lt;/h6>
&lt;ul>
&lt;li>At a concurrency level of 3500, the throughput of vertices is 261, and the amount of data processed per second is 52,200 (261 * 200).&lt;/li>
&lt;/ul>
&lt;h6 id="maximum-insertion-speed-of-edges">Maximum Insertion Speed of Edges:&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/cassandra/edge_batch.png" alt="image">
&lt;/center>
&lt;h6 id="conclusion-1">Conclusion:&lt;/h6>
&lt;ul>
&lt;li>At a concurrency level of 1000, the throughput of edges is 323, and the amount of data processed per second is 161,500 (323 * 500).&lt;/li>
&lt;/ul>
&lt;h4 id="42-single-insertion">4.2 Single Insertion&lt;/h4>
&lt;h5 id="421-pressure-upper-limit-test">4.2.1 Pressure Upper Limit Test&lt;/h5>
&lt;h6 id="test-method-1">Test Method&lt;/h6>
&lt;p>Continuously increase the concurrency level to test the upper limit of the server&amp;rsquo;s ability to provide services.&lt;/p>
&lt;h6 id="pressure-parameters-1">Pressure Parameters&lt;/h6>
&lt;ul>
&lt;li>Duration: 5 minutes.&lt;/li>
&lt;li>Service exception mark: Error rate greater than 0.00%.&lt;/li>
&lt;/ul>
&lt;h6 id="single-insertion-of-vertices">Single Insertion of Vertices:&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/cassandra/vertex_single.png" alt="image">
&lt;/center>
&lt;h6 id="conclusion-2">Conclusion:&lt;/h6>
&lt;ul>
&lt;li>At a concurrency level of 9000, the throughput is 8400, and the single-insertion concurrency capability for vertices is 9000.&lt;/li>
&lt;/ul>
&lt;h6 id="single-insertion-of-edges">Single Insertion of Edges:&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/cassandra/edge_single.png" alt="image">
&lt;/center>
&lt;h6 id="conclusion-3">Conclusion:&lt;/h6>
&lt;ul>
&lt;li>At a concurrency level of 4500, the throughput is 4160, and the single-insertion concurrency capability for edges is 4500.&lt;/li>
&lt;/ul>
&lt;h4 id="43-query-by-id">4.3 Query by ID&lt;/h4>
&lt;h5 id="431-pressure-upper-limit-test">4.3.1 Pressure Upper Limit Test&lt;/h5>
&lt;h6 id="test-method-2">Test Method&lt;/h6>
&lt;p>Continuously increase the concurrency and test the upper limit of the pressure that the server can still provide services normally.&lt;/p>
&lt;h6 id="pressure-parameters-2">Pressure Parameters&lt;/h6>
&lt;ul>
&lt;li>Duration: 5 minutes&lt;/li>
&lt;li>Service exception flag: error rate greater than 0.00%&lt;/li>
&lt;/ul>
&lt;h6 id="query-by-id-for-vertices">Query by ID for vertices&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/cassandra/vertex_id_query.png" alt="image">
&lt;/center>
&lt;h6 id="conclusion-4">Conclusion:&lt;/h6>
&lt;ul>
&lt;li>The concurrent capacity of the vertex search by ID is 14500, with a throughput of 13576 and an average delay of 11ms.&lt;/li>
&lt;/ul>
&lt;h6 id="edge-search-by-id">Edge search by ID&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.5.6/cassandra/edge_id_query.png" alt="image">
&lt;/center>
&lt;h6 id="conclusion-5">Conclusion:&lt;/h6>
&lt;ul>
&lt;li>For edge ID-based queries, the server&amp;rsquo;s concurrent capacity is up to 12,000, with a throughput of 10,688 and an average latency of 63ms.&lt;/li>
&lt;/ul></description></item><item><title>Docs: v0.4.4</title><link>/docs/performance/api-preformance/hugegraph-api-0.4.4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/performance/api-preformance/hugegraph-api-0.4.4/</guid><description>
&lt;h3 id="1-test-environment">1 Test environment&lt;/h3>
&lt;p>Target Machine Information&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>机器编号&lt;/th>
&lt;th>CPU&lt;/th>
&lt;th>Memory&lt;/th>
&lt;th>NIC (Network Interface Card)&lt;/th>
&lt;th>Disk&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>24 Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz&lt;/td>
&lt;td>61G&lt;/td>
&lt;td>1000Mbps&lt;/td>
&lt;td>1.4T HDD&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>48 Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz&lt;/td>
&lt;td>128G&lt;/td>
&lt;td>10000Mbps&lt;/td>
&lt;td>750GB SSD,2.7T HDD&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>&lt;strong>Pressure testing machine information:&lt;/strong> Configured the same as machine number 1.&lt;/li>
&lt;li>&lt;strong>Testing tool:&lt;/strong> Apache JMeter 2.5.1.&lt;/li>
&lt;/ul>
&lt;p>Note: The pressure testing machine and the machine being tested are in the same room.&lt;/p>
&lt;h3 id="2-test-description">2 Test Description&lt;/h3>
&lt;h4 id="21-definition-of-terms-the-unit-of-time-is-ms">2.1 Definition of terms (the unit of time is ms)&lt;/h4>
&lt;ul>
&lt;li>Samples &amp;ndash; The total number of threads completed in this scenario.&lt;/li>
&lt;li>Average &amp;ndash; The average response time.&lt;/li>
&lt;li>Median &amp;ndash; The median response time in terms of statistical significance.&lt;/li>
&lt;li>90% Line &amp;ndash; The response time of 90% of all threads is less than xx.&lt;/li>
&lt;li>Min &amp;ndash; The minimum response time.&lt;/li>
&lt;li>Max &amp;ndash; The maximum response time.&lt;/li>
&lt;li>Error &amp;ndash; The error rate.&lt;/li>
&lt;li>Throughput &amp;ndash; The throughput.&lt;/li>
&lt;li>KB/sec &amp;ndash; The throughput measured in terms of traffic.&lt;/li>
&lt;/ul>
&lt;h4 id="22-underlying-storage">2.2 Underlying storage&lt;/h4>
&lt;p>RocksDB is used for backend storage, HugeGraph and RocksDB are both started on the same machine, and the configuration files related to the server remain the default except for the modification of the host and port.&lt;/p>
&lt;h3 id="3-summary-of-performance-results">3 Summary of performance results&lt;/h3>
&lt;ol>
&lt;li>The upper limit of the number of requests HugeGraph can handle per second is 7000&lt;/li>
&lt;li>The speed of batch insertion is much higher than that of single insertion, and the test results on the server reach 22w edges/s, 37w vertices/s&lt;/li>
&lt;li>The backend is RocksDB, and increasing the number of CPUs and memory size can improve the performance of batch inserts. Doubling the CPU and memory size can increase performance by 45% to 60%.&lt;/li>
&lt;li>In the batch insertion scenario, using SSD instead of HDD, the performance improvement is small, only 3%-5%&lt;/li>
&lt;/ol>
&lt;h3 id="4-test-results-and-analysis">4 Test results and analysis&lt;/h3>
&lt;h4 id="41-batch-insertion">4.1 Batch insertion&lt;/h4>
&lt;h5 id="411-maximum-pressure-test">4.1.1 Maximum Pressure Test&lt;/h5>
&lt;h6 id="test-methods">Test Methods&lt;/h6>
&lt;p>Continuously increase the concurrency level and test the upper limit of the server&amp;rsquo;s ability to provide services normally.&lt;/p>
&lt;h6 id="pressure-parameters">Pressure Parameters&lt;/h6>
&lt;p>Duration: 5 minutes&lt;/p>
&lt;h6 id="maximum-insertion-speed-of-vertices-and-edges-high-performance-server-with-ssd-storage-for-rocksdb-data">Maximum Insertion Speed of Vertices and Edges (High-performance server with SSD storage for RocksDB data):&lt;/h6>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.4.4/best.png" alt="image">
&lt;/center>
&lt;h6 id="conclusion">Conclusion:&lt;/h6>
&lt;ul>
&lt;li>With a concurrency of 1000, the edge throughput is 451, which can process 225,500 data per second: 451 * 500 = 225,500/s.&lt;/li>
&lt;li>With a concurrency of 2000, the vertex throughput is 1842.4, which can process 368,480 data per second: 1842.4 * 200 = 368,480/s.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>1. The Impact of CPU and Memory on Insertion Performance (Servers Using HDD Storage for RocksDB Data, Batch Insertion)&lt;/strong>&lt;/p>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.4.4/cpu-memory.png" alt="image">
&lt;/center>
&lt;h6 id="conclusion-1">Conclusion:&lt;/h6>
&lt;ul>
&lt;li>With the same HDD disk, doubling the CPU and memory size increases edge throughput from 268 to 426, which improves performance by about 60%.&lt;/li>
&lt;li>With the same HDD disk, doubling the CPU and memory size increases vertex throughput from 1263.8 to 1842.4, which improves performance by about 45%.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>2. The Impact of SSD and HDD on Insertion Performance (High-performance Servers, Batch Insertion)&lt;/strong>&lt;/p>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.4.4/ssd.png" alt="image">
&lt;/center>
&lt;h6 id="conclusion-2">Conclusion:&lt;/h6>
&lt;ul>
&lt;li>For edge insertion, using SSD yields a throughput of 451.7, while using HDD yields a throughput of 426.6, which results in a 5% performance improvement.&lt;/li>
&lt;li>For vertex insertion, using SSD yields a throughput of 1842.4, while using HDD yields a throughput of 1794, which results in a performance improvement of about 3%.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>3. The Impact of Different Concurrent Thread Numbers on Insertion Performance (Ordinary Servers, HDD Storage for RocksDB Data)&lt;/strong>&lt;/p>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.4.4/threads-batch.png" alt="image">
&lt;/center>
&lt;h5 id="conclusion-3">Conclusion:&lt;/h5>
&lt;ul>
&lt;li>For vertices, at 1000 concurrency, the response time is 7ms and at 1500 concurrency, the response time is 1028ms. The throughput remained around 1300, indicating that the inflection point data should be around 1300. At 1300 concurrency, the response time has reached 22ms, which is within a controllable range. Compared to HugeGraph 0.2 (1000 concurrency: average response time 8959ms), the processing capacity has made a qualitative leap.&lt;/li>
&lt;li>For edges, the processing time is too long and exceeds 3 seconds from 1000 to 2000 concurrency, and the throughput almost fluctuates around 270. Therefore, increasing the concurrency will not significantly increase the throughput. 270 is an inflection point, and compared with HugeGraph 0.2 (1000 concurrency: average response time 31849ms), the processing capacity has improved significantly.&lt;/li>
&lt;/ul>
&lt;h4 id="42-single-insertion">4.2 single insertion&lt;/h4>
&lt;h5 id="421-upper-limit-test-under-pressure">4.2.1 Upper Limit Test under Pressure&lt;/h5>
&lt;h6 id="test-method">Test Method&lt;/h6>
&lt;p>Continuously increase the concurrency level and test the upper limit of the pressure at which the server can still provide normal services.&lt;/p>
&lt;h6 id="pressure-parameters-1">Pressure Parameters&lt;/h6>
&lt;ul>
&lt;li>Duration: 5 minutes&lt;/li>
&lt;li>Service exception criteria: Error rate greater than 0.00%.&lt;/li>
&lt;/ul>
&lt;center>
&lt;img src="/docs/images/API-perf/v0.4.4/threads-single.png" alt="image">
&lt;/center>
&lt;h4 id="conclusion-4">Conclusion:&lt;/h4>
&lt;ul>
&lt;li>Vertices:
&lt;ul>
&lt;li>At 4000 concurrent connections, there were no errors, with an average response time of less than 1ms. At 6000 concurrent connections, there were no errors, with an average response time of 5ms, which is acceptable.&lt;/li>
&lt;li>At 8000 concurrent connections, there were 0.01% errors and the system could not handle it, resulting in connection timeout errors. The system&amp;rsquo;s peak performance should be around 7000 concurrent connections.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Edges:
&lt;ul>
&lt;li>At 4000 concurrent connections, the response time was 1ms. At 6000 concurrent connections, there were no abnormalities, with an average response time of 8ms. The main differences were in IO network recv and send as well as CPU usage.&lt;/li>
&lt;li>At 8000 concurrent connections, there was a 0.01% error rate, with an average response time of 15ms. The turning point should be around 7000 concurrent connections, which matches the vertex results.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: v0.2</title><link>/docs/performance/api-preformance/hugegraph-api-0.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/performance/api-preformance/hugegraph-api-0.2/</guid><description>
&lt;h3 id="1-test-environment">1 Test environment&lt;/h3>
&lt;h4 id="11-software-and-hardware-information">1.1 Software and hardware information&lt;/h4>
&lt;p>The load testing and target machines have the same configuration, with the following basic parameters:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>CPU&lt;/th>
&lt;th>Memory&lt;/th>
&lt;th>网卡&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>24 Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz&lt;/td>
&lt;td>61G&lt;/td>
&lt;td>1000Mbps&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="12-service-configuration">1.2 Service Configuration&lt;/h4>
&lt;ul>
&lt;li>HugeGraph Version: 0.2&lt;/li>
&lt;li>Backend Storage: Cassandra 3.10, deployed as a single node in the service.&lt;/li>
&lt;li>Backend Configuration Modification: Modified two properties in the cassandra.yaml file, while keeping the rest of the options default:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span> batch_size_warn_threshold_in_kb: 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> batch_size_fail_threshold_in_kb: 1000
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>HugeGraphServer, HugeGremlinServer, and Cassandra are all started on the same machine. Configuration files for the servers are modified only for the host and port settings.&lt;/li>
&lt;/ul>
&lt;h4 id="13-glossary">1.3 Glossary&lt;/h4>
&lt;ul>
&lt;li>Samples &amp;ndash; The total number of threads completed in this scenario.&lt;/li>
&lt;li>Average &amp;ndash; The average response time.&lt;/li>
&lt;li>Median &amp;ndash; The statistical median of response times.&lt;/li>
&lt;li>90% Line &amp;ndash; The response time below which 90% of all threads fall.&lt;/li>
&lt;li>Min &amp;ndash; The minimum response time.&lt;/li>
&lt;li>Max &amp;ndash; The maximum response time.&lt;/li>
&lt;li>Error &amp;ndash; The error rate.&lt;/li>
&lt;li>Troughput &amp;ndash; The number of requests processed per unit of time.&lt;/li>
&lt;li>KB/sec &amp;ndash; The throughput measured in kilobytes per second.&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Note: All time units are measured in ms.&lt;/em>&lt;/p>
&lt;h3 id="2-test-results">2 Test Results&lt;/h3>
&lt;h4 id="21-schema">2.1 schema&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Label&lt;/th>
&lt;th>Samples&lt;/th>
&lt;th>Average&lt;/th>
&lt;th>Median&lt;/th>
&lt;th>90%Line&lt;/th>
&lt;th>Min&lt;/th>
&lt;th>Max&lt;/th>
&lt;th>Error%&lt;/th>
&lt;th>Throughput&lt;/th>
&lt;th>KB/sec&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>property_keys&lt;/td>
&lt;td>331000&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;td>0&lt;/td>
&lt;td>172&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>920.7/sec&lt;/td>
&lt;td>178.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vertex_labels&lt;/td>
&lt;td>331000&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>126&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>920.7/sec&lt;/td>
&lt;td>193.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>edge_labels&lt;/td>
&lt;td>331000&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>158&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>920.7/sec&lt;/td>
&lt;td>242.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Conclusion: Under the pressure of 1000 concurrent requests lasting for 5 minutes, the average response time for the schema interface is 1-2ms, and there is no pressure.&lt;/p>
&lt;h4 id="22-single-insert">2.2 Single Insert&lt;/h4>
&lt;h5 id="221-insertion-rate-test">2.2.1 Insertion Rate Test&lt;/h5>
&lt;h6 id="pressure-parameters">Pressure Parameters&lt;/h6>
&lt;p>Test Method: Fixed concurrency, test server and backend processing speed.&lt;/p>
&lt;ul>
&lt;li>Concurrency: 1000&lt;/li>
&lt;li>Duration: 5 minutes&lt;/li>
&lt;/ul>
&lt;h6 id="performance-indicators">Performance Indicators&lt;/h6>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Label&lt;/th>
&lt;th>Samples&lt;/th>
&lt;th>Average&lt;/th>
&lt;th>Median&lt;/th>
&lt;th>90%Line&lt;/th>
&lt;th>Min&lt;/th>
&lt;th>Max&lt;/th>
&lt;th>Error%&lt;/th>
&lt;th>Throughput&lt;/th>
&lt;th>KB/sec&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>single_insert_vertices&lt;/td>
&lt;td>331000&lt;/td>
&lt;td>0&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>21&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>920.7/sec&lt;/td>
&lt;td>234.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>single_insert_edges&lt;/td>
&lt;td>331000&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>53&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>920.7/sec&lt;/td>
&lt;td>309.1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h6 id="conclusion">Conclusion&lt;/h6>
&lt;ul>
&lt;li>For vertices: average response time of 1ms, with each request inserting one piece of data. With an average of 920 requests processed per second, the average total data processed per second is approximately 920 pieces of data.&lt;/li>
&lt;li>For edges: average response time of 1ms, with each request inserting one piece of data. With an average of 920 requests processed per second, the average total data processed per second is approximately 920 pieces of data.&lt;/li>
&lt;/ul>
&lt;h5 id="222-stress-test">2.2.2 Stress Test&lt;/h5>
&lt;p>Test Method: Continuously increase concurrency to test the maximum stress level at which the server can still provide normal services.&lt;/p>
&lt;h6 id="stress-parameters">Stress Parameters&lt;/h6>
&lt;ul>
&lt;li>Duration: 5 minutes&lt;/li>
&lt;li>Service Exception Flag: Error rate greater than 0.00%&lt;/li>
&lt;/ul>
&lt;h6 id="performance-metrics">Performance Metrics&lt;/h6>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Concurrency&lt;/th>
&lt;th>Samples&lt;/th>
&lt;th>Average&lt;/th>
&lt;th>Median&lt;/th>
&lt;th>90%Line&lt;/th>
&lt;th>Min&lt;/th>
&lt;th>Max&lt;/th>
&lt;th>Error%&lt;/th>
&lt;th>Throughput&lt;/th>
&lt;th>KB/sec&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>2000(vertex)&lt;/td>
&lt;td>661916&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>3012&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>1842.9/sec&lt;/td>
&lt;td>469.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4000(vertex)&lt;/td>
&lt;td>1316124&lt;/td>
&lt;td>13&lt;/td>
&lt;td>1&lt;/td>
&lt;td>14&lt;/td>
&lt;td>0&lt;/td>
&lt;td>9023&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>3673.1/sec&lt;/td>
&lt;td>935.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5000(vertex)&lt;/td>
&lt;td>1468121&lt;/td>
&lt;td>1010&lt;/td>
&lt;td>1135&lt;/td>
&lt;td>1227&lt;/td>
&lt;td>0&lt;/td>
&lt;td>9223&lt;/td>
&lt;td>0.06%&lt;/td>
&lt;td>4095.6/sec&lt;/td>
&lt;td>1046.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7000(vertex)&lt;/td>
&lt;td>1378454&lt;/td>
&lt;td>1617&lt;/td>
&lt;td>1708&lt;/td>
&lt;td>1886&lt;/td>
&lt;td>0&lt;/td>
&lt;td>9361&lt;/td>
&lt;td>0.08%&lt;/td>
&lt;td>3860.3/sec&lt;/td>
&lt;td>987.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2000(edge)&lt;/td>
&lt;td>629399&lt;/td>
&lt;td>953&lt;/td>
&lt;td>1043&lt;/td>
&lt;td>1113&lt;/td>
&lt;td>1&lt;/td>
&lt;td>9001&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>1750.3/sec&lt;/td>
&lt;td>587.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3000(edge)&lt;/td>
&lt;td>648364&lt;/td>
&lt;td>2258&lt;/td>
&lt;td>2404&lt;/td>
&lt;td>2500&lt;/td>
&lt;td>2&lt;/td>
&lt;td>9001&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>1810.7/sec&lt;/td>
&lt;td>607.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4000(edge)&lt;/td>
&lt;td>649904&lt;/td>
&lt;td>1992&lt;/td>
&lt;td>2112&lt;/td>
&lt;td>2211&lt;/td>
&lt;td>1&lt;/td>
&lt;td>9001&lt;/td>
&lt;td>0.06%&lt;/td>
&lt;td>1812.5/sec&lt;/td>
&lt;td>608.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h6 id="conclusion-1">Conclusion&lt;/h6>
&lt;ul>
&lt;li>Vertex:
&lt;ul>
&lt;li>4000 concurrency: normal, no error rate, average time 13ms;&lt;/li>
&lt;li>5000 concurrency: if 5000 data insertions are processed per second, there will be an error rate of 0.06%, indicating that it cannot be handled. The peak should be at 4000.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Edge:
&lt;ul>
&lt;li>1000 concurrency: response time is 2ms, which is quite different from the response time of 2000 concurrency, mainly because IO network rec and send, as well as CPU, have almost doubled);&lt;/li>
&lt;li>2000 concurrency: if 2000 data insertions are processed per second, the average time is 953ms, and the average number of requests processed per second is 1750;&lt;/li>
&lt;li>3000 concurrency: if 3000 data insertions are processed per second, the average time is 2258ms, and the average number of requests processed per second is 1810;&lt;/li>
&lt;li>4000 concurrency: if 4000 data insertions are processed per second, the average number of requests processed per second is 1812;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="23-batch-insertion">2.3 Batch Insertion&lt;/h4>
&lt;h5 id="231-insertion-rate-test">2.3.1 Insertion Rate Test&lt;/h5>
&lt;h6 id="pressure-parameters-1">Pressure Parameters&lt;/h6>
&lt;p>Test Method: Fix the concurrency and test the processing speed of the server and backend.&lt;/p>
&lt;ul>
&lt;li>Concurrency: 1000&lt;/li>
&lt;li>Duration: 5 minutes&lt;/li>
&lt;/ul>
&lt;h6 id="performance-indicators-1">Performance Indicators&lt;/h6>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Label&lt;/th>
&lt;th>Samples&lt;/th>
&lt;th>Average&lt;/th>
&lt;th>Median&lt;/th>
&lt;th>90%Line&lt;/th>
&lt;th>Min&lt;/th>
&lt;th>Max&lt;/th>
&lt;th>Error%&lt;/th>
&lt;th>Throughput&lt;/th>
&lt;th>KB/sec&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>batch_insert_vertices&lt;/td>
&lt;td>37162&lt;/td>
&lt;td>8959&lt;/td>
&lt;td>9595&lt;/td>
&lt;td>9704&lt;/td>
&lt;td>17&lt;/td>
&lt;td>9852&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>103.4/sec&lt;/td>
&lt;td>393.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>batch_insert_edges&lt;/td>
&lt;td>10800&lt;/td>
&lt;td>31849&lt;/td>
&lt;td>34544&lt;/td>
&lt;td>35132&lt;/td>
&lt;td>435&lt;/td>
&lt;td>35747&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>28.8/sec&lt;/td>
&lt;td>814.9&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h6 id="conclusion-2">Conclusion&lt;/h6>
&lt;ul>
&lt;li>Vertex: The average response time is 8959ms, which is too long. Each request inserts 199 data, and the average processing rate is 103 requests per second. Therefore, the average number of data processed per second is about 2w (20,000) data.&lt;/li>
&lt;li>Edge: The average response time is 31849ms, which is too long. Each request inserts 499 data, and the average processing rate is 28 requests per second. Therefore, the average number of data processed per second is about 13900 (13,900) data.&lt;/li>
&lt;/ul></description></item></channel></rss>